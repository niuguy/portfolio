{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "# For Chinese \n",
    "# -*- coding=UTF-8 -*-\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "print(tf.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the file for train from  http://www.manythings.org/anki/, \n",
    "# the corpus is related to translating from Chinese to English\n",
    "\n",
    "path_to_file = \"deu.txt\"\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    \n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w +' <end>'\n",
    "    return w\n",
    "\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = codecs.open(path, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')] for line in lines[:num_examples]]\n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "        \n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "    \n",
    "    inp_lang = LanguageIndex(en for en, de in pairs)\n",
    "    targ_lang = LanguageIndex(de for en, de in pairs)\n",
    "    \n",
    "    # Vectorize the input and target classes\n",
    "    # English sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, de in pairs]\n",
    "    \n",
    "    # German sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in de.split(' ')] for en, de in pairs]\n",
    "    \n",
    "\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 1000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 800, 200, 200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.data\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 16\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units,\n",
    "                                        return_sentence = True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # For Attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units) ## ??\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1) \n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis = 1)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def initial_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1- np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = real, logits = pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './train_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.1141\n",
      "Epoch 1 Loss 1.7264\n",
      "Time taken for 1 epoch 96.7108380795 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            dec_hidden = enc_hidden\n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)\n",
    "            \n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                loss += loss_function(targ[:,t], predictions)\n",
    "                dec_input = tf.expand_dims(targ[:,t], 1)\n",
    "                \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "         # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_tarp):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.multinomial(tf.exp(predictions), num_samples = 1)[0][0].numpy() ##?\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x119fb2dd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i will go <end>\n",
      "Predicted translation: ich bin . <end> \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Colormap viridis is not recognized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3746a7a8a83a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"I will go\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_targ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-eae9da63c698>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplot_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-219babc300ac>\u001b[0m in \u001b[0;36mplot_attention\u001b[0;34m(attention, sentence, predicted_sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfontdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'fontsize'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/matplotlib/axes.pyc\u001b[0m in \u001b[0;36mmatshow\u001b[0;34m(self, Z, **kwargs)\u001b[0m\n\u001b[1;32m   9108\u001b[0m               'aspect': 'equal'}          # (already the imshow default)\n\u001b[1;32m   9109\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9110\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_top\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/matplotlib/axes.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   7296\u001b[0m         im = mimage.AxesImage(self, cmap, norm, interpolation, origin, extent,\n\u001b[1;32m   7297\u001b[0m                        \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7298\u001b[0;31m                        filterrad=filterrad, resample=resample, **kwargs)\n\u001b[0m\u001b[1;32m   7299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7300\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/matplotlib/image.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ax, cmap, norm, interpolation, origin, extent, filternorm, filterrad, resample, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                                 \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                                 \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m                                 )\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/matplotlib/image.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ax, cmap, norm, interpolation, origin, filternorm, filterrad, resample, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[1;32m     88\u001b[0m         \u001b[0mmartist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArtist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalarMappable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morigin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/matplotlib/cm.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, norm, cmap)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m#: The Colormap instance of this ScalarMappable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;31m#: The last colorbar associated with this ScalarMappable. May be None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/matplotlib/cm.pyc\u001b[0m in \u001b[0;36mget_cmap\u001b[0;34m(name, lut)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_cmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Colormap %s is not recognized\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Colormap viridis is not recognized"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJPCAYAAAC+fJpMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAE2JJREFUeJzt3V+oZeVhxuF319GAUGtlwAudMDCRJBaUJEQnDSE7KHTiRQdSqJg0kBqoN6a9KTXmojk3afGmhCAYsUZ6FS+SXJgiStpkkyL+BeOfxhFnWmFmDCFJ2xBKoDN4erG2ejzOzF5nznvOnD0+DwzsddY3az74GP2x1rfXJAAAAAAAAAAAAAAAAAAAAACs860kP0/ywhnGfCPJK0meS/Kh7ZgUAMAy+kSGWDpdWN2U5OH55+uTPLEdkwIAWFZ7c/qw+maSm9ccH0py+VZPCABgp/mdwjWuSHJ0zfGxJFcWrgsAsFQaYZUkk3XHq6XrAgAsjV2FaxxPsmfN8ZXzn73Nvn37Vo8cOVL44wAAttyRJO/b6G9qhNVDSW5P8mCS/Un+J8O3CN/myJEjWV11I2tZraysZGVl5VxPg7Ng7Zab9Vte1m65TSaTfWfz+8aE1beTfDLJ7gx7qb6a5ML5uXszfCPwpiSHk/xvkj8/m4kAACy7MWF1y4gxt292IgAAy661eZ3z3HQ6PddT4CxZu+Vm/ZaXtXt3Wv9tvq20ao8VALAMJpNJchad5I4VAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUjAmrA0kOJXklyR2nOL87ySNJfpLkxSRfaE0OAGCZTBacvyDJy0luTHI8ydNJbkny0poxK0nek+TODJH1cpLLk5xcd63V1dXVzc8YAGCLTSaTZHEnvcOiO1bXJTmc5NUkJ5I8mOTgujE/S3LJ/PMlSX6Vd0YVAMB5b9eC81ckObrm+FiS69eNuS/JD5O8luR3k/xpbXYAAEtkUViNeXb3lQz7q6ZJ9iX5QZJrk/xm/cCVlZU3P0+n00yn03GzBADYQrPZLLPZbNPXWfTscH+GPVQH5sd3Jnk9yV1rxjyc5GtJHpsf/2uGTe7PrLuWPVYAwFLYqj1WzyS5KsneJBcluTnJQ+vGHMqwuT0ZNq2/P8l/bHQiAADLbtGjwJNJbk/yaIZvCN6f4RuBt83P35vk75I8kOS5DKH2N0n+aysmCwCwk234FtcmeBQIACyFrXoUCADASMIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKxoTVgSSHkryS5I7TjJkmeTbJi0lmjYkBACybyYLzFyR5OcmNSY4neTrJLUleWjPm0iSPJfmjJMeS7E7yy1Nca3V1dXWz8wUA2HKTySRZ3EnvsOiO1XVJDid5NcmJJA8mObhuzGeTfDdDVCWnjioAgPPeorC6IsnRNcfH5j9b66oklyX5UZJnkny+NjsAgCWya8H5Mc/uLkzy4SQ3JLk4yeNJnsiwJwsA4F1jUVgdT7JnzfGevPXI7w1HMzz+++3814+TXJtThNXKysqbn6fTaabT6UbnCwBQN5vNMpvNNn2dRZuydmXYvH5DkteSPJV3bl7/QJK7M2xef0+SJ5PcnOSn665l8zoAsBTOdvP6ojtWJ5PcnuTRDN8QvD9DVN02P39vhlcxPJLk+SSvJ7kv74wqAIDz3oZLbBPcsQIAlsJWvW4BAICRhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAkjFhdSDJoSSvJLnjDOM+muRkks8U5gUAsHQWhdUFSe7OEFdXJ7klyQdPM+6uJI8kmTQnCACwLBaF1XVJDid5NcmJJA8mOXiKcV9K8p0kv2hODgBgmSwKqyuSHF1zfGz+s/VjDia5Z3682pkaAMByWRRWYyLp60m+PB87iUeBAMC71K4F548n2bPmeE+Gu1ZrfSTDI8Ik2Z3k0xkeGz60/mIrKytvfp5Op5lOpxuaLADAVpjNZpnNZpu+zqK7S7uSvJzkhiSvJXkqwwb2l04z/oEk30/yvVOcW11d9ZQQANj5JpNJchZP4RbdsTqZ5PYkj2b45t/9GaLqtvn5ezf6BwIAnK+2cz+UO1YAwFI42ztW3rwOAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUjA2rA0kOJXklyR2nOP+5JM8leT7JY0muqcwOAGCJTEaMuSDJy0luTHI8ydNJbkny0poxH0vy0yS/zhBhK0n2r7vO6urq6ianCwCw9SaTSTKuk95mzB2r65IcTvJqkhNJHkxycN2YxzNEVZI8meTKjU4EAGDZjQmrK5IcXXN8bP6z0/likoc3MykAgGW0a8SYjTy/+1SSW5N8/FQnV1ZW3vw8nU4znU43cGkAgK0xm80ym802fZ0xzw73Z9gzdWB+fGeS15PctW7cNUm+Nx93+BTXsccKAFgKW7nH6pkkVyXZm+SiJDcneWjdmPdmiKo/y6mjCgDgvDfmUeDJJLcneTTDNwTvz/CNwNvm5+9N8rdJfj/JPfOfnciw6R0A4F1jw7e4NsGjQABgKWzlo0AAAEYQVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUDImrA4kOZTklSR3nGbMN+bnn0vyoc7UAACWy6KwuiDJ3Rni6uoktyT54LoxNyV5X5KrkvxFknvKc2QHmM1m53oKnCVrt9ys3/Kydu9Oi8LquiSHk7ya5ESSB5McXDfmj5P80/zzk0kuTXJ5b4rsBP4Dsbys3XKzfsvL2r07LQqrK5IcXXN8bP6zRWOu3PzUAACWy6KwWh15nclZ/j4AgPPG+iBab3+SlQx7rJLkziSvJ7lrzZhvJplleEyYDBvdP5nk5+uudTjJvrOfKgDAtjmSYQ951a75hfcmuSjJT3LqzesPzz/vT/JEexIAAOeLTyd5OcMdpzvnP7tt/usNd8/PP5fkw9s6OwAAAABYxAtFl9eitftchjV7PsljSa7Zvqkxwpi/e0ny0SQnk3xmOybFKGPWbprk2SQvZtjXys6xaP12J3kkw3aaF5N8YdtmxiLfyrAn/IUzjDmnzXJBhkeCe5NcmMV7sq6PPVk7xZi1+1iS35t/PhBrt5OMWb83xv0wyT8n+ZPtmhxnNGbtLk3y73nrVTa7t2tyLDRm/VaS/P388+4kv8qwh5lz7xMZYul0YbXhZmn/W4FeKLq8xqzd40l+Pf/8ZLyvbCcZs35J8qUk30nyi22bGYuMWbvPJvluhvcEJskvt2tyLDRm/X6W5JL550syhNXJbZofZ/ZvSf77DOc33CztsPJC0eU1Zu3W+mLeqnjOvbF/9w7mrX92yvvmdoYxa3dVksuS/CjJM0k+vz1TY4Qx63dfkj9I8lqGx0l/tT1To2DDzdK+FemFostrI2vwqSS3Jvn4Fs2FjRuzfl9P8uX52EkWv8eO7TFm7S7M8I3rG5JcnOHu8RMZ9n1wbo1Zv69keEQ4zfA+xx8kuTbJb7ZuWhRtqFnaYXU8yZ41x3vy1q3r0425cv4zzq0xa5cMG9bvy7DH6ky3T9leY9bvI3nrRb67M7xK5USSh7Z8dpzJmLU7muHx32/nv36c4X/MwurcG7N+f5jka/PPR5L8Z5L3Z7j7yM52zpvFC0WX15i1e2+GvQT7t3VmjDFm/dZ6IL4VuFOMWbsPJPmXDBulL86w0fbq7ZsiZzBm/f4hyVfnny/PEF6XbdP8WGxvxm1eP2fN4oWiy2vR2v1jhk2Xz85/PbXdE+SMxvzde4Ow2lnGrN1fZ/hm4AtJ/nJbZ8cii9Zvd5LvZ/h/3gsZvozAzvDtDHvf/i/DneFbo1kAAAAAAAAAAAAAAAAAAAAAAAAAAGB5/T+C/j6M1WB4dwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119eccf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u\"I will go\", encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
